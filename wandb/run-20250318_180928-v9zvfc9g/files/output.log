Number of train_response logs 58926
[Epoch 1]
Training:   0%|                                                             | 0/231 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "d:\Git\IDCDF-WKY\run.py", line 65, in <module>
    sys.exit(main(config_dict))
             ^^^^^^^^^^^^^^^^^
  File "d:\Git\IDCDF-WKY\run.py", line 62, in main
    idcdm.train(datahub, "train", "valid", valid_metrics=validate_metrics, batch_size=config['batch_size'],epoch=config['epoch'], lr=config['lr'])
  File "d:\Git\IDCDF-WKY\inscd\models\idcdm.py", line 45, in train
    self._train(datahub=datahub, set_type=set_type,
  File "d:\Git\IDCDF-WKY\inscd\_base.py", line 50, in _train
    unifier.train(datahub, set_type, self.extractor, self.inter_func, **kwargs)
  File "d:\Git\IDCDF-WKY\inscd\_unifier.py", line 31, in train
    _ = extractor.extract(student_id,exercise_id,r_matrix)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Git\IDCDF-WKY\inscd\extractor\idcd.py", line 57, in extract
    self.student_ts.requires_grad=False
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().
